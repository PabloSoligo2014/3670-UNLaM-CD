{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KSLgqOSknCdK"},"source":["# Caso: Predicción de Tiempo de Espera en Caja\n","\n","## Objetivo\n","Predecir el tiempo de espera a ser atendido en caja en una sucursal bancaria."]},{"cell_type":"code","metadata":{"id":"_7hH3gmMnCdQ"},"source":["# Librerias a importar\n","import math as math\n","import numpy as np\n","import pandas as pd\n","import pydotplus\n","import seaborn as sns\n","import copy\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","\n","%matplotlib inline\n","\n","from IPython.display import Image\n","\n","from sklearn.tree import export_graphviz\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.metrics import make_scorer\n","from sklearn import metrics\n","\n","from sklearn.svm import SVR\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","\n","from sklearn import datasets, linear_model\n","from sklearn.preprocessing import LabelEncoder\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-OXbsXF9EYw"},"source":["## 1. Análisis descriptivo del dataset\n","\n","El set de datos a utilizar es **espera_caja.csv** el cual contiene información sobre atenciónes en caja de una sucursal bancaria.\n","\n","La variable target es **ESPERA_SEQ1**, que especifica el tiempo de espera que tuvo la persona para ser atendida."]},{"cell_type":"code","metadata":{"id":"79dQ8E5Tx2Vf"},"source":["# Leemos el dataset\n","dataset_entrenamiento='https://raw.githubusercontent.com/unlam-fcdin/UNLaM_FCDIN/master/espera_caja.csv'\n","df = pd.read_csv(dataset_entrenamiento, sep =',', na_values = '.')\n","\n","# Para estandarizar el análisis, llamaremos variable \"CLASE\" a la variable target\n","df['CLASE'] = df.apply(lambda x: None if x['ESPERA_SEQ1'] is None or math.isnan(x['ESPERA_SEQ1']) else int(x['ESPERA_SEQ1']), axis=1)\n","df.drop([\"ESPERA_SEQ1\"], axis=1, inplace=True)\n","\n","print(df.shape)\n","df.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnZgajaueLns"},"source":["#### **Descripción Estadística General**\n","Veamos qué información relevante obtenemos de este primer análisis"]},{"cell_type":"code","metadata":{"id":"INR7F23-PsbR"},"source":["# ¿Qué información estadística obtenemos del dataset?\n","df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLjeVnAcPsbW"},"source":["# Veamos la distribución de la variable target\n","print(round(df['CLASE'].mean()))     # Media\n","print(round(df['CLASE'].median()))   # Mediana\n","print(round(df['CLASE'].std()))      # Desvío\n","\n","plt.figure(figsize=(20,10))\n","sns.distplot((df)['CLASE'], hist=True, bins=100, color = 'darkblue',\n","             hist_kws={'edgecolor':'black'},\n","             kde_kws={'linewidth': 4})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1UxHm6dyPqu"},"source":["### 1.a) Análisis de Missing Values o Nulos\n","\n","Analizamos la existencia de valores faltantes y seleccionamos la estrategia de imputación."]},{"cell_type":"code","metadata":{"id":"ROBQ2X_xyvyV"},"source":["val_nulos = df.isnull().sum()\n","print(df.shape)\n","print(val_nulos)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFQBS_YKAnUO"},"source":["# Borramos los casos donde la clase es null, ya que no sirven para entrenamiento\n","print(df.shape)\n","df = df[~df.CLASE.isnull()]\n","print(df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3VulhZ0ye3T"},"source":["# Graficamos la distribución de valores nulos en cada variable\n","atributos_nulos = (df.isnull()).sum(axis=0)\n","atributos_nulos = pd.DataFrame(atributos_nulos, columns=['Cantidad de Nulos'])\n","atributos_nulos = atributos_nulos.sort_values(by=['Cantidad de Nulos'], ascending=True)\n","atributos_nulos.drop(['CLASE'], inplace = True)\n","atributos_nulos.plot(kind='barh', figsize=(15,5), color='orange', grid=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NU5F75C9y6DD"},"source":["### 1.b) Análisis de Outliers\n","\n","Analizamos los valores outliers de cada variable y determinamos si queremos/debemos imputarlos o no."]},{"cell_type":"code","metadata":{"id":"b61oUPobzCad"},"source":["# Creamos un gráfico boxplot para analizar las variables\n","for c in df.columns:\n","  if(\"float\" in str(df.dtypes[c]) or \"int\" in str(df.dtypes[c])):\n","    plt.figure(figsize=(10, 3))\n","    sns.boxplot(x=c, data=df, orient = 'h', palette=\"Set2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ew9DvUP1WFN8"},"source":["## 2. Preparación de datos\n","\n","Una vez cargados los datos y analizados, se deben preparar para ser procesados. Los 3 pasos generales que se deben seguir son:\n","\n","1. Tratamiento de outliers\n","2. Feature engineering\n","3. Tratamiento de missing values (o valores nulos)\n","\n","Los pasos detallados de la fase, son:\n","\n","- Leer los datos con Pandas.\n","- Comprobar si hay valores nulos y crear todas las variables nuevas.\n","- Construir una variable Y que contenga la variable objetivo a predecir y una vector X que contenga todo el resto de variables a usar en la predicción.\n","- Dividir el dataset completo en training y testing\n","- Dividir X e y con train_test_split así:\n","        train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","Vamos a crear una función llamada **preparacion_de_datos(dataset)** que incluirá todas las tareas de preparación de datos necesarias para construir nuestro modelo predictivo. Esta función la vamos a utilizar tanto para el dataset de entrenamiento como para el de prueba.\n"]},{"cell_type":"code","metadata":{"id":"5EOGactuWh2x"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","# FUNCION preparacion_de_datos (:parametros)\n","#   df_e            => Dataset de entrada a modificar\n","#   imputar_ouliers => Flag que indica si se deben imputar outliers o no\n","#   imputar_nulos   => Flag que indica si se deben imputar nulos o no\n","\n","def preparacion_de_datos(df_e, imputar_ouliers, imputar_nulos):\n","\n","  # Comenzamos haciendo una copia del dataset que la función recibe como parámetro de entrada\n","  df_s = copy.copy(df_e)\n","\n","  # (*1) ---- IMPUTACIÓN DE OUTLIERS ----\n","\n","  # En este punto se deben imputar los valores outliers, sólo si se indicó por parámetro(imputar_outliers)\n","  if imputar_ouliers:\n","    print(\"TODO: Imputación de outliers.\")\n","\n","    # Primero definimos una función para calcular la media, con los parámetros:\n","    ## dff -> Dataframe a usar\n","    ## c   -> Columna a imputar\n","    ## min -> Límite inferior, si no aplica no se envía.\n","    ## max -> Límite superior, si no aplica no se envía.\n","\n","    def calcular_media(dff, c, min=None, max=None):\n","      # Seteamos el mínimo y máximo, si viene por parámetro. Si no fueron informados, tomamos el actual del set de datos.\n","      minimo = dff[c].min() if min==None else min\n","      maximo = dff[c].max() if max==None else max\n","\n","      # Filtramos los registros dentro del rango\n","      dff2 = dff[(dff[c]>=minimo) & (dff[c]<=maximo)]\n","\n","      # Devolvemos la media\n","      return dff2[c].mean()\n","\n","    # Imputamos por la media los outliers superiores\n","    outlier_superior = df_s['VISITAS_CAJA'].mean() + 1.5*df_s['VISITAS_CAJA'].std()\n","    media_sin_outliers = calcular_media(df_s, 'VISITAS_CAJA', max=outlier_superior)\n","    df_s['VISITAS_CAJA'] = df_s.apply(lambda x: media_sin_outliers if x['VISITAS_CAJA']>outlier_superior else x['VISITAS_CAJA'], axis=1)\n","\n","    # Imputamos por la media los outliers superiores\n","    outlier_superior = df_s['FRECUENCIA_ENCUESTAS'].mean() + 2*df_s['FRECUENCIA_ENCUESTAS'].std()\n","    media_sin_outliers = calcular_media(df_s, 'FRECUENCIA_ENCUESTAS', max=outlier_superior)\n","    df_s['FRECUENCIA_ENCUESTAS'] = df_s.apply(lambda x: media_sin_outliers if x['FRECUENCIA_ENCUESTAS']>outlier_superior else x['FRECUENCIA_ENCUESTAS'], axis=1)\n","\n","  # (*2) ---- FEATURE ENGINEERING ----\n","\n","  # Cada uno puede crear los atributos que considere necesario (y mejoren la predicción)\n","  print(\"TODO: Creación de nuevas variables.\")\n","\n","  # (*3) ---- TRATAMIENTO DE VALORES NULOS ----\n","\n","  # Veamos si tenemos valores nulos o infinitos. Como ejemplo, podemos optar por setear el valor 0 por defecto.\n","  if imputar_nulos:\n","    print(\"TODO: Imputación de valores nulos.\")\n","\n","    # Al resto los imputo por la media\n","    df_s[df_s==np.inf]=np.nan\n","    for c in df_s.columns:\n","      if(not(\"str\"  in str(df_s.dtypes[c]) or \"object\" in str(df_s.dtypes[c]))):\n","        df_s[c].fillna(df_s[c].mean(), inplace=True)\n","\n","  return df_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbLmSw5qnCda"},"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","# Ejecutamos la función de preparacion de datos.\n","# Indicando que si queremos imputar ouliers y si queremos imputar nulos\n","# Este el primer paso que debemos realizar para tener todas las variables a utilizar.\n","df = preparacion_de_datos(df, True, True)\n","X = pd.get_dummies(df.drop([\"CLASE\"],axis = 1),  dtype=int)\n","\n","# Escalamos algunos de los atributos para que estén entre 0 y 1\n","atributos = X.columns\n","#num_vars = ['Desc_Resp_Cerrada','EDAD','FRECUENCIA_ENCUESTAS','VISITAS_CAJA']\n","#X[atributos] = scaler.fit_transform(X[atributos])\n","\n","y = df['CLASE']\n","\n","X.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rm4tfXDtPscW"},"source":["# Dividimos el dataset en entrenamiento y prueba (70% para training y 30% para testing)\n","# Dividimos X e y con la funcion train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.3,\n","                                                    random_state=42)\n","\n","print(X_train.shape)\n","X_train.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELQOIudUFYXh"},"source":["#### **Preparación de Datos**\n","**Consignas**\n","- ¿Se observa algo objetable en el set de datos generado?\n","\n","- ¿Hay alguna variable que requiera un tratamiento adicional?\n","\n","\n","Cabe resaltar que cualquier actividad a realizar sobre el set de datos, debe realizarse en la fase de **Preparación de Datos**."]},{"cell_type":"markdown","metadata":{"id":"9fWxA_rlbYqS"},"source":["### Análisis de Correlación de Variables Numéricas\n","\n","Una matriz de correlación permite estudiar la relación lineal o comportamiento que puede existir entre dos o más variables.\n","\n","  - Correlación positiva: ocurre cuando una variable aumenta y la otra también.\n","  - Correlación negativa: es cuando una variable aumenta y la otra disminuye.\n","  - Sin correlación: no hay una relación aparente entre las variables."]},{"cell_type":"code","metadata":{"id":"KgJXmGDybcEM"},"source":["# Seleccionamos sólo la correlación de la variable objetivo numérica CLASE.\n","X_trainw = copy.copy(X_train)\n","X_trainw[\"CLASE\"] = y_train\n","dfd = X_trainw.corr()[[\"CLASE\"]]*100\n","del X_trainw\n","\n","# Borramos la correlación de la variable objetivo numérica consigo misma.\n","dfd = dfd.drop(\"CLASE\", axis=0)\n","\n","# Ordenamos las variables de forma decreciente por el valor de correlación positiva con la variable objetivo.\n","dfd = dfd.sort_values([\"CLASE\"], ascending=False)\n","\n","dfd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAt1dDTZG0Wd"},"source":["## 3. Construcción del modelo predictivo de regresión lineal sin optimizacion de hiperparámetros\n","### 3.a) Modelo inicial\n","\n","Vamos a construir un modelo de regresión lineal inicial usando el datasetde entrenamiento. Los pasos que realizaremos son:\n","\n","- Ajustar un modelo regresión lineal con parámetros `default`.\n","- Calcular la importancia de los atributos.\n","- Calcular y mostrar el error cometido por el modelo.\n","- Quitar variables con baja importancia y ver si la regresión mejora."]},{"cell_type":"code","metadata":{"id":"bKwWYjyunCdd"},"source":["# Ajustamos una Regresión Lineal con los parametros default\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISt53tzYnCdg"},"source":["### 3.b) Interpretación de resultados\n","- La interpretación de la regresión, se realiza en los coeficientes de las variables utilizadas.\n","- Estos coeficientes dan una idea de la importancia de cada variable para el modelo generado."]},{"cell_type":"code","metadata":{"id":"IzWPbfeJnbSl"},"source":["pd.DataFrame({'Atributo':atributos,\n","              'importancia':(lr.coef_)}).sort_values('importancia',\n","                                                        ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtWPJGjAE2fh"},"source":["El método antiguo"]},{"cell_type":"code","metadata":{"id":"ijVDjaswE1-9"},"source":["import statsmodels.api as sm\n","from scipy import stats\n","\n","X2 = sm.add_constant(X_train)\n","est = sm.OLS(y_train, X2)\n","est2 = est.fit()\n","\n","print(est2.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7RuY6Ui8Psco"},"source":["### 3.c) Evaluación de modelos\n","En el caso de Regresiones, para evaluar qué modelo es mejor/peor, vamos a evaluar la distancia del valor predicho sobre el valor real. En este caso cuantos segundos por exceso, o por defecto, predecimos vs los segundos reales de espera.\n","\n","Con el modelo ya entrenado, utilizamos el conjunto de prueba para verificar su capacidad de predicción. Existen diversas métricas para esta medición, pero una de las más utilizadas es la métrica de Error Cuadrático Medio (**MSE**) o su versión ajustada **RMSE** (Raíz del Error Cuadrático Medio), cuyas fórmulas son:\n","\n","$$MSE = {\\frac{\\sum_{t=1}^n (y_t-\\hat y_t )^2}{n}}$$  \n","$$RMSE = \\sqrt{\\frac{\\sum_{t=1}^n (y_t-\\hat y_t )^2}{n}}$$"]},{"cell_type":"code","metadata":{"id":"wSz_lVcko66T"},"source":["# Realizamos la predicción sobre el set de testing\n","y_pred = lr.predict(X_test)\n","\n","# MSE - Error cuadratico medio\n","print('Mean squared error: %.4f'% mean_squared_error(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IpSAc5G6LYvX"},"source":["# Intentamos calcular a mano el error cuadratico medio para ver que coincide\n","N   = X_test.shape[0]\n","MSE = (1/N) * (sum([math.pow(diff,2) for diff in y_test - y_pred]))\n","\n","print('MSE MANUAL: %.4f'% MSE)\n","print('RMSE MANUAL: %.4f'% math.sqrt(MSE))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5NZj9AsWNhR7"},"source":["#### Distribución de los errores"]},{"cell_type":"markdown","metadata":{"id":"USCLgOdrMvlG"},"source":["Como una conclusión de este análisis, es que nuestro modelo falla en un rango de **13.5 segundos** en promedio dentro de todo el set de datos.\n","\n","En algunos casos este fallo es mayor y en otros menos. Veamos la distribución de los errores:"]},{"cell_type":"code","metadata":{"id":"ksWGJ2-hNPDR"},"source":["errores = [diff for diff in y_test - (y_pred-4)]\n","\n","plt.figure(figsize=(20,10))\n","sns.distplot(errores, hist=True, kde=True,\n","             bins=50, color = 'darkblue',\n","             hist_kws={'edgecolor':'black'},\n","             kde_kws={'linewidth': 4})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B82MluTQJFSf"},"source":["#### Coeficiente de determinación R2\n","Por otro lado, para entender que poder predictivo tiene el modelo podemos analizar el coeficiente de determinación o R2, el cual nos indica que % de la varianza total de la variable objetivo, es explicada por el modelo generado.\n","\n","El valor 1 simboliza que el modelo explica el 100% de la varianza del set de datos, lo cual indicaría que es un modelo sobre ajustado, valores muy bajos indican que es un modelo sub-ajustado o con variables no relevantes para la predicción. Lo ideal es usar el coeficiente de determinación ajustado, ya que considera un trade off entre la mejora y la cantidad de variables agregadas, en función de la cantidad de registros del set de datos.\n","\n","![texto alternativo](https://i.stack.imgur.com/fLrDw.png)"]},{"cell_type":"code","metadata":{"id":"AU2kd_vTJli9"},"source":["# Calculo el coeficiente de determinación R2\n","r2 = r2_score(y_test, y_pred)\n","\n","# Calculo el R2 ajustado\n","n = X_test.shape[0]\n","p = len(X_train.columns)\n","r2_ajustado = 1 - ( (1-r2) * ( (n-1) / ( n-p-1 ) ) )\n","\n","# Muestro los coeficientes\n","print('Coeficiente de determinación R2: %.4f'% r2)\n","print('Coeficiente de determinación ajustado: %.4f'% r2_ajustado)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2b4LuP9OcpY"},"source":["#### Quitamos variables no relevantes\n","Veamos como cambia el modelo si quitamos las variables no interesantes al objetivo de predicción.\n"]},{"cell_type":"code","metadata":{"id":"q00K7aB_O6nf"},"source":["var_ok = ['MODELO_ATENCION_RENTA_MASIVA','MODELO_ATENCION_JOVENES','MODELO_ATENCION_PREMIUM',\n","          'MODELO_ATENCION_EMPRESAS','Desc_Resp_Cerrada','VISITAS_CAJA','MODELO_ATENCION_NORMAL']\n","\n","# Quitamos las columnas que no interesan\n","X_train2 = X_train[var_ok]\n","X_test2 = X_test[var_ok]\n","\n","# Entrenamos y realizamos la predicción sobre el set de testing\n","lr = LinearRegression()\n","lr = lr.fit(X_train2, y_train)\n","y_pred = lr.predict(X_test2)\n","\n","# MSE - Error cuadratico medio\n","print('MSE: %.5f'% mean_squared_error(y_test, y_pred))\n","print('RMSE: %.5f'% math.sqrt(mean_squared_error(y_test, y_pred)))\n","\n","# Calculo el coeficiente de determinación R2\n","r2 = r2_score(y_test, y_pred)\n","\n","# Calculo el R2 ajustado\n","n = X_test2.shape[0]\n","p = len(X_test2.columns)\n","r2_ajustado = 1 - ( (1-r2) * ( (n-1) / ( n-p-1 ) ) )\n","\n","# Muestro los coeficientes\n","print('Coeficiente de determinación R2: %.4f'% r2)\n","print('Coeficiente de determinación ajustado: %.4f'% r2_ajustado)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"opMkxQuedU0t"},"source":["#### Distribución de los errores\n","Vemos que la distribución de los errores mejora."]},{"cell_type":"code","metadata":{"id":"T_Dy12CldQbE"},"source":["errores = [diff for diff in y_test - (y_pred)]\n","\n","media_errores = np.mean(errores)\n","mediana_errores = np.median(errores)\n","\n","print(\"Media de los errores: \"+str(media_errores))\n","print(\"Mediana de los errores: \"+str(mediana_errores))\n","\n","plt.figure(figsize=(20,10))\n","sns.distplot(errores, hist=True, kde=True,\n","             bins=100, color = 'darkblue',\n","             hist_kws={'edgecolor':'black'},\n","             kde_kws={'linewidth': 4})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJfna6qhnCd0"},"source":["## 4. Construcción del modelo usando GridSearchCV con optimización de hiperparámetros\n","\n","Las clase **GridSearchCV** se utiliza para automatizar la selección de los parámetros de un modelo, aplicando para ello la técnica de validación cruzada. Partiendo de un modelo y un conjunto de parámetros, GridSearchCV prueba múltiples combinaciones y selecciona los valores de los parámetros que ofrecen mayor rendimiento para un modelo y conjunto de datos.\n","\n","Los parámetros a optimizar son:\n","\n","Medida            | Que hace\n","------------------|-------------\n","alpha         | Indica el nivel de ajuste a realizar, cuanto mayor menos overfitting\n","fit_intercept      | Indica si se quiere agregar o no intercepto\n","max_iter    | Maxima cantidad de iteraciones para converger\n","tol    | Nivel de tolerancia\n","normalize  | Indica si es necesario normalizar los datos\n","solver | Tipo de algoritmo interno a aplicar\n","\n","Cada uno puede definir sus propios rangos de valores para cada parámetro, considerando que cuantos más parámetros se usen, el tiempo de procesamiento crece exponencialmente."]},{"cell_type":"markdown","metadata":{"id":"OKL5f87mPscx"},"source":["### 4.a) Definición de parámetros"]},{"cell_type":"code","metadata":{"id":"CBkvFvsHSkwg"},"source":["model = Ridge()\n","model.get_params().keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNb6h3_jnCd3"},"source":["# Definimos los parametros a evaluar:\n","\n","PARAMETROS = {\"fit_intercept\":[True, False],\n","              \"tol\":          [0.00001, 0.00005, 0.0001, 0.0005],\n","              \"max_iter\":     [100, 200, 1000],\n","              \"alpha\":        [0.001, 0.01, 0.05, 0.1, 1, 10]}\n","\n","k_n_jobs = 1 # numero de jobs a ejecutar en paralelo\n","\n","# Hacemos la búsqueda con GridSearchCV\n","model = Ridge()\n","#model = Lasso()\n","gs = GridSearchCV(model,\n","                  PARAMETROS,\n","                  n_jobs=k_n_jobs,\n","                  refit=True,\n","                  scoring='neg_root_mean_squared_error', #Definimos la metrica RMSE Raiz del Error Cuadrático Medio\n","                  cv=KFold(n_splits=10,shuffle=True,random_state=1), #Cross Validation de 5 capas\n","                  verbose=1)\n","gs.fit(X_train2, y_train)\n","\n","# Mostramos los mejores resultados obtenidos\n","print(gs.best_estimator_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3g0eSz0QUtf"},"source":["# Uso el mejor modelo para predecir\n","y_pred = gs.best_estimator_.predict(X_test2)\n","\n","# MSE - Error cuadratico medio\n","print('Puntaje del modelo en CV: {:.5f}'.format(gs.best_score_))\n","print('Puntaje del modelo en TESTING: {:.5f}'.format(math.sqrt(mean_squared_error(y_test, y_pred))))\n","\n","# Calculo el coeficiente de determinación R2\n","r2 = r2_score(y_test, y_pred)\n","\n","# Calculo el R2 ajustado\n","n = X_test2.shape[0]\n","p = len(X_test2.columns)\n","r2_ajustado = 1 - ( (1-r2) * ( (n-1) / ( n-p-1 ) ) )\n","\n","# Muestro los coeficientes\n","print('Coeficiente de determinación R2: %.4f'% r2)\n","print('Coeficiente de determinación ajustado: %.4f'% r2_ajustado)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDEsnfzOASz2"},"source":["Para ver el resultado final, reentrenamos al modelo y mostramos en un dataframe la comparación entre los valores reales, los predichos y su diferencia"]},{"cell_type":"code","metadata":{"id":"a7PXmwMjARST"},"source":["val_real = pd.Series(y_test.values)\n","val_pred = pd.Series(y_pred)\n","\n","predicciones = pd.concat([val_real.rename('Valor real'),val_pred.rename('Valor Pred') ,abs(val_real-val_pred).rename('Dif(+/-)')] ,  axis=1)\n","predicciones = predicciones.sort_values(by='Dif(+/-)')\n","predicciones.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicciones.tail(20)"],"metadata":{"id":"ydPpjMDqBErd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leeBC3H2IIUX"},"source":["### 4.b) Podemos definir una métrica propia para considerar Acierto o Error\n","Por ejemplo, consideramos error las predicciones donde el error es superior a X%"]},{"cell_type":"code","metadata":{"id":"AISLmEk4hScZ"},"source":["predicciones.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUV5IBi7E5a4"},"source":["predicciones['ERROR_PORC'] = predicciones['Dif(+/-)'] / predicciones['Valor real']\n","print(\"Errores: \"+str(predicciones[predicciones['ERROR_PORC'] > 0.7].shape[0] / predicciones.shape[0]))\n","print(\"Total: \"+str(predicciones.shape[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYYYkdHxX-nI"},"source":["##**Tips**:\n","### Versión antigua de Regresion Lineal\n","Para conocer la relevancia o no de las variables (en función del p-value) se puede usar la libreria original de stadísticas en python **statsmodels**:\n","\n","    import statsmodels.api as sm\n","    from scipy import stats\n","\n","    X2 = sm.add_constant(X_train)\n","    est = sm.OLS(y_train, X2)\n","    est2 = est.fit()\n","\n","    print(est2.summary())\n","\n","### Uso de Google Colab\n","Si se desea usar el poder de computo de Google Colab sin necesidad de estar sentado en la notebook/pc, se pueden realizar los siguientes pasos:\n","- Poner a ejecutar todas las celdas: \"Entorno de ejecución\" -> \"Ejecutar todas\"\n","- Activar la consola de Google Chrome presionando F12.\n","- Ir a la pestaña Console.\n","- Ejecutar el siguiente código javascript:\n","\n","      function ConnectButton(){\n","        console.log(\"Connect pushed\");\n","        document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect-icon\").click()\n","      }\n","      setInterval(ConnectButton,60000);\n","- Esto va a presionar continuamente el botón conectar del Colab y va a mantener la sesión activa."]},{"cell_type":"code","source":[],"metadata":{"id":"O9_QmwuXHuwy"},"execution_count":null,"outputs":[]}]}