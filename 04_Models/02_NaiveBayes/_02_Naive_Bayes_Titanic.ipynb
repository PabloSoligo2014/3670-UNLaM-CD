{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KSLgqOSknCdK"},"source":["# Clasificadores Bayesianos\n","\n","## Caso: Titanic\n","\n","En el hundimiento del Titanic murieron 1514 personas de las 2223 que iban a bordo, lo que convierte a esta tragedia en uno de los mayores naufragios de la historia. Vamos a utilizar un dataset (csv, separado por compas) el cual contiene un listado de 1309 pasajeros que estuvieron a bordo del Titanic para analizar la supervivencia de los pasajeros según ciertas caracteristicas (sexo, edad, ticket de clase, entre otras)."]},{"cell_type":"markdown","metadata":{"id":"0M7wfK1DnCdP"},"source":["## Análisis Exploratorio y Descriptivo\n","\n","*   Instalar todas las librerias python que aquí se incluyen.\n","*   Explorar los datos con Pandas, ordenándolos o graficándolos.\n"]},{"cell_type":"code","metadata":{"id":"_7hH3gmMnCdQ"},"source":["# Importamos las librerías que necesitamos\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pydotplus\n","import pickle\n","from sklearn.model_selection import GridSearchCV\n","\n","# Matriz de confusión\n","from sklearn.metrics import confusion_matrix\n","\n","# Library & dataset\n","import seaborn as sns\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargamos el dataset\n","df = pd.read_csv('https://raw.githubusercontent.com/PabloSoligo2014/3670-UNLaM-CD/refs/heads/main/datasets/titanic.csv', sep = ',')\n","\n","# No vamos a utilizar el PassengerId,Name porque son de tipo ID que no tienen ningun uso como variables predictoras para un modelo de clasificacion\n","df.drop(['PassengerId', 'Name'], axis=1, inplace=True)\n","\n","# Por lo pronto vamos a quitar las variables Ticket/Cabin que son variables categoricas de alta dimensionalidad\n","# en una clase a posteriori vamos a buscar la mejor forma de tratar estas variables para ser usadas en metodos de clasificacion\n","df.drop(['Ticket', 'Cabin'], axis=1, inplace=True)\n","\n","df.head(10)"],"metadata":{"id":"0yhFJ91dJiZB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Siempre el primer paso para analizar cualquier problema de clasificacion, es analizar la distribucion de la variable target."],"metadata":{"id":"ADSJJTj7Kz3I"}},{"cell_type":"code","metadata":{"id":"AP4QDb2IkR1h"},"source":["df['Survived'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DoUIUI9XYNPB"},"source":["Analicemos con un gráfico de barras, la relación entre los clientes que entran en mora de los que no, según los siguientes aspectos:\n","\n","*   Por tipo de empleo"]},{"cell_type":"code","metadata":{"id":"SJM6hbAlnCdW"},"source":["pd.crosstab(index=df['Sex'],\n","            columns=df['Survived'],\n","            margins=False).plot.barh(stacked=True,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGeAcLt4ffpA"},"source":["Esto mismo podríamos verlo con una tabla de doble entrada:"]},{"cell_type":"code","metadata":{"id":"51LobUwufwJe"},"source":["pd.crosstab(index=df['Sex'],\n","            columns=df['Survived'],\n","            margins=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kOZPrp0bZBx2"},"source":["\n","*   Por clase del boleto (1ra, 2da, etc)."]},{"cell_type":"code","metadata":{"id":"Svf097OQKYW8"},"source":["pd.crosstab(index=df['Pclass'],\n","            columns=df['Survived'],\n","            margins=False).plot.barh(stacked=True,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5MksijeZjhp"},"source":["*   Por cantidad de hermanxs"]},{"cell_type":"code","metadata":{"id":"ncMqJ8pEKYW_"},"source":["pd.crosstab(index=df['SibSp'],\n","            columns=df['Survived'],\n","            margins=False).plot.barh(stacked=True,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ju1LQ6W8gX80"},"source":["¿Cómo es la distribución de las edades de los clientes?"]},{"cell_type":"code","metadata":{"id":"C3e0ikSrKYXC"},"source":["# Construimos un gráfico de densidad\n","plt.title('Age')\n","sns.kdeplot(df['Age'], shade=True) # shade indica si el gráfico es sombreado o no"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jp9ZIuyGKYXG"},"source":["val_nulos = df.isnull().sum()\n","print(val_nulos)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKivJ7mfF3KV"},"source":["#### **Transformaciones**\n","\n","Encodeamos como booleanos todos los atributos categóricos a utilizar (sin incluir la variable clase). Para ello, aplicamos el método `pd.get_dummies`."]},{"cell_type":"code","metadata":{"id":"-5xEO8h54gPc"},"source":["df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbLmSw5qnCda"},"source":["# Armamos las variables predictoras para el algoritmo\n","X = pd.get_dummies(df.drop('Survived', axis=1))\n","atributos = X.columns\n","\n","X.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBG_wSs2ICOO"},"source":["Encodeamos las etiquetas del atributo clase usando el método `LabelEncoder`.  Este método convierte una variable categórica a numérica, con valores entre \"*0 and nro_clases-1*\", para simplificar los cálculos que hace el algoritmo.\n","\n","En nuestro caso los valores para el atributo clase serán 0 (MORA) y 1 (NO MORA)."]},{"cell_type":"code","metadata":{"id":"6RDpqQI4HK5O"},"source":["# Separamos la variable target en un atributo generico y\n","y = df['Survived']\n","y[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Od0hHojvK9U2"},"source":["#### **Partición del conjunto de datos**\n","\n","Dividimos el dataset original en dos conjuntos de datos:\n","\n","*  Conjunto de entrenamiento (70%)\n","*  Conjunto de prueba (30%)"]},{"cell_type":"code","metadata":{"id":"ZjRyb5bqHLFJ"},"source":["# Importamos la librería que necesitamos\n","from sklearn.model_selection import train_test_split\n","\n","# Dado que el clasificador Naive Bayes no puede trabajar con valores numericos nulos, vamos a asignar un valor default de 0\n","# Como se ya se vio en clases previas, este no es el camino y hay que hacer un apropiado analysis/tratamiento de valoes nulos para cada variable\n","# y definir la estrategia a utilizar\n","X.fillna(0, inplace=True)\n","\n","# Dividimos X e y con train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","print(f\"X: {X.shape} - X_train:{X_train.shape} - X_test:{X_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VG3S8mwGnNLZ"},"source":["## B. Clasificador Naïve Bayes\n","\n","En esta parte aprenderemos cómo aplicar un modelo **Naïve Bayes** a problemas de clasificación. Para ello, utilizaremos el dataset de clientes anterior y construiremos un modelo para predecir el mismo objetivo, es decir, si un cliente va a caer en mora en algun momento de la vida del mismo.\n","\n","#### Referencia: [Documentación de Naïve Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n","\n","#### **Aplicación del algoritmo de Naïve Bayes**\n","\n","Con los datos ya particionados vamos a entrenar dos modelos utilizando dos variantes del algoritmo Naïve Bayes, el multinomial y el gausiano.  \n","\n","## 1. Construcción y evaluación del primer modelo\n","\n","Para este modelo usaremos el algoritmo ***Naive Bayes “multinomial”***. Este clasificador es adecuado cuando se tienen características discretas, siendo uno de los algoritmos estándar usado por ejemplo para clasificación o categorización de texto.\n"]},{"cell_type":"code","metadata":{"id":"1VSSOX9hsRIZ"},"source":["# Importamos la librería que necesitamos\n","from sklearn.naive_bayes import MultinomialNB # naive bayes multinomial para clasificación\n","\n","# Creamos y entrenamos el clasificador bayesiano\n","bayes_multi = MultinomialNB()\n","bayes_multi.fit(X_train, y_train) # entrenamos el clasificador"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jngVL5bBdN7P"},"source":["Como ejemplo, podemos guardar el modelo entrenado en un archivo para volver a cargarlo cuando necesitemos clasificar registros nuevos"]},{"cell_type":"code","metadata":{"id":"l0Jw4YyLdH8r"},"source":["print(\"Antes del dump el objeto era: \")\n","print(bayes_multi)\n","\n","# Dejo espacio para no confundir\n","print(\" \"*30)\n","print(\"-\"*30)\n","print(\" \"*30)\n","\n","# Guardo el objeto en un archivo llamado \"modelo.bayes_multi\"\n","pickle.dump(bayes_multi, open('modelo.bayes_multi', 'wb'))\n","\n","# Ahora levanto el objeto desde el archivo guardado, de nuevo en la misma variable dic\n","bayes_multi = pickle.load(open('modelo.bayes_multi', 'rb'))\n","\n","print(\"Luego del load el objeto es: \")\n","print(bayes_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xH19WFqxqVwh"},"source":["Ahora que nuestro primer modelo ha sido entrenado, podemos utilizar el *conjunto de prueba* para verificar su capacidad de predicción."]},{"cell_type":"code","metadata":{"id":"GW_Y08u6qtbV"},"source":["# Calculamos y mostramos la matriz de confusión del modelo\n","y_pred_multi = bayes_multi.predict(X_test)\n","conf = confusion_matrix(y_test, y_pred_multi)\n","\n","predicted_cols = ['pred_'+str(c) for c in y.unique()]\n","real_cols = ['real_'+str(c) for c in y.unique()]\n","pd.DataFrame(conf, index=real_cols, columns = predicted_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculamos las metricas para evaluación de modelos de clasificación automatizadas.\n","\n","Recordemos que:\n","- **Exactitud (Accuracy)** = TP+TN / (TP + TN + FP + FN)           \n","- **Error de Predicción** = 1 - Exactitud\n","- **Precisión** = TP / (TP + FP)\n","- **Sensibilidad (Recall)** = TP / (TP + FN)\n","<br>\n","<br>"],"metadata":{"id":"DaUzg_HmPZYu"}},{"cell_type":"code","source":["# Reporte del clasificador\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred_multi))"],"metadata":{"id":"2JHClJT7Pf4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iqOQEQ94gwzz"},"source":["## 2. Construcción y evaluación del segundo modelo\n","Para el segundo modelo, probaremos con el algoritmo ***Naive Bayes \"gausiano\"***. Este algoritmo es más adecuado para datos continuos ya que asume que los datos tienen una distribución de curva de Gauss (normal).\n","\n","Entrenamos el nuevo modelo propuesto para este caso:"]},{"cell_type":"code","metadata":{"id":"wLowxfa3g7_n"},"source":["# Importamos la librería que necesitamos\n","from sklearn.naive_bayes import GaussianNB # naive bayes multinomial para clasificación\n","\n","bayes_gauss = GaussianNB()\n","bayes_gauss.fit(X_train, y_train) # entrenamos el clasificador"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l47HgcpyhMgb"},"source":["Con el segundo modelo ya entrenado, utilizamos el *conjunto de prueba* para verificar su capacidad de predicción."]},{"cell_type":"code","metadata":{"id":"EAODW_3MhWph"},"source":["# Calculamos y mostramos la matriz de confusión del modelo\n","y_pred_gauss = bayes_gauss.predict(X_test)\n","conf = confusion_matrix(y_test, y_pred_gauss)\n","\n","predicted_cols = ['pred_'+str(c) for c in y.unique()]\n","real_cols = ['real_'+str(c) for c in y.unique()]\n","pd.DataFrame(conf, index=real_cols, columns = predicted_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reporte del clasificador\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred_gauss))"],"metadata":{"id":"SBmJlhoxQLuW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bksjaVdtmnNM"},"source":["## C: Optimización con GridSearchCV\n","\n","### Modelo Naïve Bayes\n","\n","Los clasificadores Naïve Bayes ofrecen diversos hiper-parámetros para cada uno de los algoritmos que incluyen. Vamos a utilizar los parámetros para el algoritmo Gaussiano, utilizado en el primer modelo, con el cual se obtuvieron mejores estimaciones."]},{"cell_type":"code","metadata":{"id":"TwzsxLvfiMTB"},"source":["# Realizamos la búsqueda con Grid Search para el modelo Gaussiano\n","model = GaussianNB() # modelo a utilizar\n","parametros = {'var_smoothing': np.logspace(0, -9, num=100)}\n","\n","# Tambien podrian probar multinomial cuyo hyperparametro es alpha\n","# model = MultinomialNB() # modelo a utilizar\n","# parametros = {'alpha':[0.0001, 0.001, 0.05, 0.1, 1]}\n","\n","# En este caso vamos a buscar la mejor configuracion del modelo para optimizar la metrica **precision** pueden elegir la que mejor aplique a su problema\n","gs = GridSearchCV(model, parametros, scoring='precision', verbose=1 , n_jobs=-1)\n","gs.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"le38XJmDtyPL"},"source":["**Nota**: El hiper-parámetro ***var_smoothing*** se utiliza para realizar un suavizado en la distribucion de las variables predictoras, aplicando un filtro en comparacion con la distribucion gaussiana. Es un metodo de regularizacion para entrenar un model mas estable y reducir el overfitting.\n","- siempre podemos validar los hiperparametros de cada modelo en la documentacion de scikit learn: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n","\n","**Nota**: El hiper-parámetro ***alpha*** se utiliza para establecer el suavizado de Laplace, para que la probabilidad resultante nunca sea cero y al menos el dato aparezca una vez. Valores mayores fomentan el underfitting."]},{"cell_type":"markdown","metadata":{"id":"VUU4_hscnUir"},"source":["Mostramos los mejores resultados obtenidos a partir de los hiper-parámetros utilizados."]},{"cell_type":"code","metadata":{"id":"T0kMC60YnXXS"},"source":["print(gs.best_estimator_)\n","print(gs.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ljXRAu6vqahQ"},"source":["Utilizando el *conjunto de prueba* y los mejores hiper-parámetros seleccionados, verificamos la capacidad de predicción del modelo obtenido."]},{"cell_type":"code","metadata":{"id":"4KkTzhb0qf4Q"},"source":["# Calculamos y mostramos la matriz de confusión del modelo\n","y_pred = gs.best_estimator_.predict(X_test)\n","conf = confusion_matrix(y_test, y_pred)\n","\n","predicted_cols = ['pred_'+str(c) for c in y.unique()]\n","real_cols = ['real_'+str(c) for c in y.unique()]\n","pd.DataFrame(conf, index=real_cols, columns = predicted_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reporte del clasificador\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"metadata":{"id":"jaajB4v3QZ9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9ONJiJyKSkxh"},"execution_count":null,"outputs":[]}]}